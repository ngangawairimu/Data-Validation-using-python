{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f662d169",
   "metadata": {},
   "source": [
    "# Validating our data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af890c",
   "metadata": {},
   "source": [
    "I will be diving into the agricultural dataset for majindogo dataset  to continue to validate  data for usage. Before  that, I want  to build a data pipeline that will ingest and clean our data with the press of a button, cleaning up our code significantly. Once that’s ready, I will  complete my data validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944ccbc",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e633f",
   "metadata": {},
   "source": [
    "From previous automated farm analysis, I will pick up from there.\n",
    "\n",
    "My  previous disappointment - was the mismatch between our dataset and the weather station data. That was a curveball, and half of our measurements were out of range, raising eyebrows and doubts alike. \n",
    "\n",
    "\n",
    "quick summary of previous analysis -  half of the means were not within that tolerance\n",
    "\n",
    "**Hypothesis testing** taking  into account both the means and the variances of the distributions being compared. The variance here is crucial because it gives us insight into the spread of the data points around the means for our two datasets. Two samples could have the same mean but very different variances, leading to different interpretations of their similarities or differences.\n",
    "\n",
    "My main goal is the same: Is the data in our `MD_agric_df` dataset representative of reality? To answer this, I use weather-related data from nearby stations to validate the results. If the weather data matches the data we have, now one  can be more confident that our dataset represents reality. \n",
    "\n",
    "So my plan is  \n",
    "1. Create a null hypothesis.\n",
    "1. Import the `MD_agric_df` dataset and clean it up.\n",
    "1. Import the weather data.\n",
    "1. Map the weather data to the field data.\n",
    "1. Calculate the means of the weather station dataset and the means of the main dataset.\n",
    "2. Calculate all the parameters we need to do a t-test. \n",
    "3. Interpret our results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147be850",
   "metadata": {},
   "source": [
    "# Data dictionary\n",
    "### from Explore AI Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8e55e",
   "metadata": {},
   "source": [
    "**1. Geographic features**\n",
    "\n",
    "- **Field_ID:** A unique identifier for each field (BigInt).\n",
    " \n",
    "- **Elevation:** The elevation of the field above sea level in metres (Float).\n",
    "\n",
    "- **Latitude:** Geographical latitude of the field in degrees (Float).\n",
    "\n",
    "- **Longitude:** Geographical longitude of the field in degrees (Float).\n",
    "\n",
    "- **Location:** Province the field is in (Text).\n",
    "\n",
    "- **Slope:** The slope of the land in the field (Float).\n",
    "\n",
    "**2. Weather features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Rainfall:** Amount of rainfall in the area in mm (Float).\n",
    "\n",
    "- **Min_temperature_C:** Average minimum temperature recorded in Celsius (Float).\n",
    "\n",
    "- **Max_temperature_C:** Average maximum temperature recorded in Celsius (Float).\n",
    "\n",
    "- **Ave_temps:** Average temperature in Celcius (Float).\n",
    "\n",
    "**3. Soil and crop features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Soil_fertility:** A measure of soil fertility where 0 is infertile soil, and 1 is very fertile soil (Float).\n",
    "\n",
    "- **Soil_type:** Type of soil present in the field (Text).\n",
    "\n",
    "- **pH:** pH level of the soil, which is a measure of how acidic/basic the soil is (Float).\n",
    "\n",
    "**4. Farm management features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Pollution_level:** Level of pollution in the area where 0 is unpolluted and 1 is very polluted (Float).\n",
    "\n",
    "- **Plot_size:** Size of the plot in the field (Ha) (Float).\n",
    "\n",
    "- **Chosen_crop:** Type of crop chosen for cultivation (Text).\n",
    "\n",
    "- **Annual_yield:** Annual yield from the field (Float). This is the total output of the field. The field size and type of crop will affect the Annual Yield\n",
    "\n",
    "- **Standard_yield:** Standardised yield expected from the field, normalised per crop (Float). This is independent of field size, or crop type. Multiplying this number by the field size, and average crop yield will give the Annual_Yield.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Weather_station_data (CSV)**\n",
    "\n",
    "- **Weather_station_ID:** The weather station the data originated from. (Int)\n",
    "\n",
    "- **Message:** The weather data was captured by sensors at the stations, in the format of text messages.(Str)\n",
    "\n",
    "**Weather_data_field_mapping (CSV)**\n",
    "\n",
    "- **Field_ID:** The id of the field that is connected to a weather station. This is the key we can use to join the weather station ID to the original data. (Int)\n",
    "\n",
    "- **Weather_station_ID:** The weather station that is connected to a field. If a field has `weather_station_ID = 0` then that field is closest to weather station 0. (Int)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cca3f5",
   "metadata": {},
   "source": [
    "# import pandas\n",
    "### To avoid errors I will   use python 3.12  and install pyarrow if warning exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33c121c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ddc37",
   "metadata": {},
   "source": [
    "We can safely ignore these warnings, but soon our script will fail to import Pandas, so let's fix it today, and we won't have to worry about it for a long time. The warning tells us that Pyarrow will soon be a requirement to import Pandas, so we can just install it with pip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab50c3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: Pyarrow in c:\\users\\admin\\anaconda3\\envs\\ds_explore\\lib\\site-packages (15.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in c:\\users\\admin\\anaconda3\\envs\\ds_explore\\lib\\site-packages (from Pyarrow) (1.26.3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install Pyarrow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7c0e7",
   "metadata": {},
   "source": [
    "# Cleaning up our data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a858a84",
   "metadata": {},
   "source": [
    "data set pulled in the data last time, there was an assumption that our script worked. assuming  that all the fixes  made to the data made it in, and assumed that the database didn't change. But what if someone added more records, fixed the data on the database  added a new column of data? \n",
    "\n",
    "\n",
    "Last time in automating farm analysis,I use below code to  imported data  (⚠️ Don't run it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff3fd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # importing the Pandas package with an alias, pd\n",
    "from sqlalchemy import create_engine, text # Importing the SQL interface. If this fails, run !pip install sqlalchemy in another cell.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Create an engine for the database\n",
    "engine = create_engine('sqlite:///Maji_Ndogo_farm_survey_small.db') #Make sure to have the .db file in the same directory as this notebook, and the file name matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "# Create a connection object\n",
    "with engine.connect() as connection:\n",
    "    \n",
    "    # Use Pandas to execute the query and store the result in a DataFrame\n",
    "    MD_agric_df = pd.read_sql_query(text(sql_query), connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e4173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MD_agric_df.rename(columns={'Annual_yield': 'Crop_type_Temp', 'Crop_type': 'Annual_yield'}, inplace=True)\n",
    "MD_agric_df.rename(columns={'Crop_type_Temp': 'Crop_type'}, inplace=True)\n",
    "MD_agric_df['Elevation'] = MD_agric_df['Elevation'].abs()\n",
    "\n",
    "# Correcting 'Crop_type' column\n",
    "def correct_crop_type(crop):\n",
    "    crop = crop.strip()  # Remove trailing spaces\n",
    "    corrections = {\n",
    "        'cassaval': 'cassava',\n",
    "        'wheatn': 'wheat',\n",
    "        'teaa': 'tea'\n",
    "    }\n",
    "    return corrections.get(crop, crop)  # Get the corrected crop type, or return the original if not in corrections\n",
    "\n",
    "# Apply the correction function to the Crop_type column\n",
    "MD_agric_df['Crop_type'] = MD_agric_df['Crop_type'].apply(correct_crop_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df10cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\")\n",
    "weather_station_mapping_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97717c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Importing the regex pattern\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "patterns = {\n",
    "    'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "     'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "    'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "    }\n",
    "\n",
    "def extract_measurement(message):\n",
    "    \"\"\"\n",
    "    Extracts a numeric measurement value from a given message string.\n",
    "\n",
    "    The function applies regular expressions to identify and extract\n",
    "    numeric values related to different types of measurements such as\n",
    "    Rainfall, Average Temperatures, and Pollution Levels from a text message.\n",
    "    It returns the key of the matching record, and first matching value as a floating-point number.\n",
    "    \n",
    "    Parameters:\n",
    "    message (str): A string message containing the measurement information.\n",
    "\n",
    "    Returns:\n",
    "    float: The extracted numeric value of the measurement if a match is found;\n",
    "           otherwise, None.\n",
    "\n",
    "    The function uses the following patterns for extraction:\n",
    "    - Rainfall: Matches numbers (including decimal) followed by 'mm', optionally spaced.\n",
    "    - Ave_temps: Matches numbers (including decimal) followed by 'C', optionally spaced.\n",
    "    - Pollution_level: Matches numbers (including decimal) following 'Pollution at' or '='.\n",
    "    \n",
    "    Example usage:\n",
    "    extract_measurement(\"【2022-01-04 21:47:48】温度感应: 现在温度是 12.82C.\")\n",
    "    # Returns: 'Temperature', 12.82\n",
    "    \"\"\"\n",
    "    \n",
    "    for key, pattern in patterns.items(): # Loop through all of the patterns and check if it matches the pattern value.\n",
    "        match = re.search(pattern, message)\n",
    "        if match:\n",
    "            # Extract the first group that matches, which should be the measurement value if all previous matches are empty.\n",
    "            # print(match.groups()) # Uncomment this line to help you debug your regex patterns.\n",
    "            return key, float(next((x for x in match.groups() if x is not None)))\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# The function creates a tuple with the measurement type and value into a Pandas Series\n",
    "result = weather_station_df['Message'].apply(extract_measurement)\n",
    "\n",
    "# Create separate columns for 'Measurement' and 'extracted_value' by unpacking the tuple with Lambda functions.\n",
    "weather_station_df['Measurement'] = result.apply(lambda x: x[0])\n",
    "weather_station_df['Value'] = result.apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d829c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function creates a tuple with the measurement type and value into a Pandas Series\n",
    "result = weather_station_df['Message'].apply(extract_measurement)\n",
    "\n",
    "# Create separate columns for 'Measurement' and 'extracted_value' by unpacking the tuple with Lambda functions.\n",
    "weather_station_df['Measurement'] = result.apply(lambda x: x[0])\n",
    "weather_station_df['Value'] = result.apply(lambda x: x[1])\n",
    "\n",
    "weather_station_means = weather_station_df.groupby(by = ['Weather_station_ID','Measurement'])['Value'].mean(numeric_only = True)\n",
    "weather_station_means = weather_station_means.unstack()\n",
    "weather_station_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e22228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this line of code to see which messages are not assigned yet.\n",
    "weather_station_df[(weather_station_df['Measurement'] == None)|(weather_station_df['Value'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MD_agric_df = MD_agric_df.merge(weather_station_mapping_df,on = 'Field_ID', how='left')\n",
    "MD_agric_df.drop(columns=\"Unnamed: 0\")\n",
    "MD_agric_df_weather_means = MD_agric_df.groupby(\"Weather_station\").mean(numeric_only = True)[['Pollution_level','Rainfall', 'Ave_temps']]\n",
    "\n",
    "MD_agric_df_weather_means = MD_agric_df_weather_means.rename(columns = {'Ave_temps':\"Temperature\"})\n",
    "MD_agric_df_weather_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b59f3d6",
   "metadata": {},
   "source": [
    "The above code one have to copy that code across from another notebook, and one may miss some blocks or lines of code or not copy over the code in the right order, and ne may not have documented the code well either. \n",
    "\n",
    "As a final step, Will  automate a few simple data validation checks in our code.\n",
    "\n",
    "\n",
    "creating  a module to interact with the database, a module to transform and clean the field-related data and another module to process the weather data.\n",
    "\n",
    "<br>\n",
    "\n",
    "So here's the plan: \n",
    "\n",
    "1. Gather all of the code from our last \"pipeline\".\n",
    "\n",
    "2. Re-organise the code into our new three modules: \n",
    "\n",
    "    a. `data_ingesation.py` - All SQL-related functions, and web-based data retrieval.\n",
    "\n",
    "    b. `field_data_processor.py` - All transformations, cleanup, and merging functionality.\n",
    "\n",
    "    c. `weather_data_processor.py` - All transformations and cleanup of the weather station data.\n",
    "\n",
    "3. Copy our code into the modules and test their functionality.\n",
    "\n",
    "4. Create automated data validation tests to ensure our data is as we expect it to be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6a0d5b",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8a491c",
   "metadata": {},
   "source": [
    "To reduce our data pipeline code to a couple of lines like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c68f47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'field_data_processor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfield_data_processor\u001b[49m\u001b[38;5;241m.\u001b[39mprocess()\n\u001b[0;32m      2\u001b[0m field_df \u001b[38;5;241m=\u001b[39m field_processor\u001b[38;5;241m.\u001b[39mdf\n\u001b[0;32m      4\u001b[0m weather_processor\u001b[38;5;241m.\u001b[39mprocess()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'field_data_processor' is not defined"
     ]
    }
   ],
   "source": [
    "field_processor.process()\n",
    "field_df = field_processor.df\n",
    "\n",
    "weather_processor.process()\n",
    "field_df = field_processor.weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b766dd0",
   "metadata": {},
   "source": [
    "The benefit of validating your data If one wants to debug a problem in the field data, we know where to go, and if one want to import other IoT weather sensors one can just modify the weather data module.\n",
    "\n",
    "The first challenge; automating the data ingestion. There are two places we're fetching data:\n",
    "1. SQLite database - creating an SQLite engine, connecting to the database, running a query, and returning a pandas DataFrame.\n",
    "2. Web CSV file - Read the CSV data from the web, and import it as a DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e22624",
   "metadata": {},
   "source": [
    "## Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9a92a",
   "metadata": {},
   "source": [
    "SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b0f0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # importing the Pandas package with an alias, pd\n",
    "from sqlalchemy import create_engine, text # Importing the SQL interface. If this fails, run !pip install sqlalchemy in another cell.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Create an engine for the database\n",
    "engine = create_engine('sqlite:///Maji_Ndogo_farm_survey_small.db') #Make sure to have the .db file in the same directory as this notebook, and the file name matches.\n",
    "\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "# Create a connection object\n",
    "with engine.connect() as connection:\n",
    "    \n",
    "    # Use Pandas to execute the query and store the result in a DataFrame\n",
    "    MD_agric_df = pd.read_sql_query(text(sql_query), connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6700c2c",
   "metadata": {},
   "source": [
    "CSV files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809e9d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\")\n",
    "weather_station_mapping_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7947076",
   "metadata": {},
   "source": [
    " **convert the data ingestion code into functions** that one can call from the module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e6de7",
   "metadata": {},
   "source": [
    "So this code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "940d7b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an engine for the database\n",
    "engine = create_engine('sqlite:///Maji_Ndogo_farm_survey_small.db') #Make sure to have the .db file in the same directory as this notebook, and the file name matches.\n",
    "\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "# Create a connection object\n",
    "with engine.connect() as connection:\n",
    "    \n",
    "    # Use Pandas to execute the query and store the result in a DataFrame\n",
    "    MD_agric_df = pd.read_sql_query(text(sql_query), connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9248b4e",
   "metadata": {},
   "source": [
    "The function for the above  code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ea12a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_engine(db_path):\n",
    "    engine = create_engine(db_path)\n",
    "    return engine\n",
    "\n",
    "def query_data(engine, sql_query):\n",
    "    with engine.connect() as connection:\n",
    "        df = pd.read_sql_query(text(sql_query), connection)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258e138",
   "metadata": {},
   "source": [
    "So if we call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a0d4832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(sqlite:///Maji_Ndogo_farm_survey_small.db)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_db_engine('sqlite:///Maji_Ndogo_farm_survey_small.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bff0102",
   "metadata": {},
   "source": [
    " SQL engine object which one can use with the query to connect to the database, and run a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55b0125d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field_ID</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Location</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Min_temperature_C</th>\n",
       "      <th>Max_temperature_C</th>\n",
       "      <th>Ave_temps</th>\n",
       "      <th>Soil_fertility</th>\n",
       "      <th>Soil_type</th>\n",
       "      <th>pH</th>\n",
       "      <th>Pollution_level</th>\n",
       "      <th>Plot_size</th>\n",
       "      <th>Crop_type</th>\n",
       "      <th>Annual_yield</th>\n",
       "      <th>Standard_yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40734</td>\n",
       "      <td>786.05580</td>\n",
       "      <td>-7.389911</td>\n",
       "      <td>-7.556202</td>\n",
       "      <td>Rural_Akatsi</td>\n",
       "      <td>14.795113</td>\n",
       "      <td>1125.2</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>33.1</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>6.169393</td>\n",
       "      <td>8.526684e-02</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.751354</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.577964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30629</td>\n",
       "      <td>674.33410</td>\n",
       "      <td>-7.736849</td>\n",
       "      <td>-1.051539</td>\n",
       "      <td>Rural_Sokoto</td>\n",
       "      <td>11.374611</td>\n",
       "      <td>1450.7</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>30.6</td>\n",
       "      <td>13.35</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.676648</td>\n",
       "      <td>3.996838e-01</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.069865</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.486302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39924</td>\n",
       "      <td>826.53390</td>\n",
       "      <td>-9.926616</td>\n",
       "      <td>0.115156</td>\n",
       "      <td>Rural_Sokoto</td>\n",
       "      <td>11.339692</td>\n",
       "      <td>2208.9</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>28.4</td>\n",
       "      <td>13.30</td>\n",
       "      <td>0.69</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.331993</td>\n",
       "      <td>3.580286e-01</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.208801</td>\n",
       "      <td>tea</td>\n",
       "      <td>0.649647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5754</td>\n",
       "      <td>574.94617</td>\n",
       "      <td>-2.420131</td>\n",
       "      <td>-6.592215</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>7.109855</td>\n",
       "      <td>328.8</td>\n",
       "      <td>-5.8</td>\n",
       "      <td>32.2</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.54</td>\n",
       "      <td>Loamy</td>\n",
       "      <td>5.328150</td>\n",
       "      <td>2.866871e-01</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.277635</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.532348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14146</td>\n",
       "      <td>886.35300</td>\n",
       "      <td>-3.055434</td>\n",
       "      <td>-7.952609</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>55.007656</td>\n",
       "      <td>785.2</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>14.25</td>\n",
       "      <td>0.72</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.721234</td>\n",
       "      <td>4.319027e-02</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.832614</td>\n",
       "      <td>wheat</td>\n",
       "      <td>0.555076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5649</th>\n",
       "      <td>11472</td>\n",
       "      <td>681.36145</td>\n",
       "      <td>-7.358371</td>\n",
       "      <td>-6.254369</td>\n",
       "      <td>Rural_Akatsi</td>\n",
       "      <td>16.213196</td>\n",
       "      <td>885.7</td>\n",
       "      <td>-4.3</td>\n",
       "      <td>33.4</td>\n",
       "      <td>14.55</td>\n",
       "      <td>0.61</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.741063</td>\n",
       "      <td>3.286828e-01</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.609930</td>\n",
       "      <td>potato</td>\n",
       "      <td>0.554482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>19660</td>\n",
       "      <td>667.02120</td>\n",
       "      <td>-3.154559</td>\n",
       "      <td>-4.475046</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>2.397553</td>\n",
       "      <td>501.1</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>32.1</td>\n",
       "      <td>13.65</td>\n",
       "      <td>0.54</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.445833</td>\n",
       "      <td>1.602583e-01</td>\n",
       "      <td>8.7</td>\n",
       "      <td>3.812289</td>\n",
       "      <td>maize</td>\n",
       "      <td>0.438194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>41296</td>\n",
       "      <td>670.77900</td>\n",
       "      <td>-14.472861</td>\n",
       "      <td>-6.110221</td>\n",
       "      <td>Rural_Hawassa</td>\n",
       "      <td>7.636470</td>\n",
       "      <td>1586.6</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>33.4</td>\n",
       "      <td>14.80</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.385873</td>\n",
       "      <td>8.221326e-09</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.681629</td>\n",
       "      <td>tea</td>\n",
       "      <td>0.800776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>33090</td>\n",
       "      <td>429.48840</td>\n",
       "      <td>-14.653089</td>\n",
       "      <td>-6.984116</td>\n",
       "      <td>Rural_Hawassa</td>\n",
       "      <td>13.944720</td>\n",
       "      <td>1272.2</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>34.6</td>\n",
       "      <td>14.20</td>\n",
       "      <td>0.63</td>\n",
       "      <td>Silt</td>\n",
       "      <td>5.562508</td>\n",
       "      <td>6.917245e-10</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.659874</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.507595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>8375</td>\n",
       "      <td>763.09030</td>\n",
       "      <td>-4.317028</td>\n",
       "      <td>-6.344461</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>35.189430</td>\n",
       "      <td>516.4</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>29.6</td>\n",
       "      <td>12.90</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.087792</td>\n",
       "      <td>2.612715e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.226532</td>\n",
       "      <td>wheat</td>\n",
       "      <td>0.453064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5654 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Field_ID  Elevation   Latitude  Longitude        Location      Slope  \\\n",
       "0        40734  786.05580  -7.389911  -7.556202    Rural_Akatsi  14.795113   \n",
       "1        30629  674.33410  -7.736849  -1.051539    Rural_Sokoto  11.374611   \n",
       "2        39924  826.53390  -9.926616   0.115156    Rural_Sokoto  11.339692   \n",
       "3         5754  574.94617  -2.420131  -6.592215  Rural_Kilimani   7.109855   \n",
       "4        14146  886.35300  -3.055434  -7.952609  Rural_Kilimani  55.007656   \n",
       "...        ...        ...        ...        ...             ...        ...   \n",
       "5649     11472  681.36145  -7.358371  -6.254369    Rural_Akatsi  16.213196   \n",
       "5650     19660  667.02120  -3.154559  -4.475046  Rural_Kilimani   2.397553   \n",
       "5651     41296  670.77900 -14.472861  -6.110221   Rural_Hawassa   7.636470   \n",
       "5652     33090  429.48840 -14.653089  -6.984116   Rural_Hawassa  13.944720   \n",
       "5653      8375  763.09030  -4.317028  -6.344461  Rural_Kilimani  35.189430   \n",
       "\n",
       "      Rainfall  Min_temperature_C  Max_temperature_C  Ave_temps  \\\n",
       "0       1125.2               -3.1               33.1      15.00   \n",
       "1       1450.7               -3.9               30.6      13.35   \n",
       "2       2208.9               -1.8               28.4      13.30   \n",
       "3        328.8               -5.8               32.2      13.20   \n",
       "4        785.2               -2.5               31.0      14.25   \n",
       "...        ...                ...                ...        ...   \n",
       "5649     885.7               -4.3               33.4      14.55   \n",
       "5650     501.1               -4.8               32.1      13.65   \n",
       "5651    1586.6               -3.8               33.4      14.80   \n",
       "5652    1272.2               -6.2               34.6      14.20   \n",
       "5653     516.4               -3.8               29.6      12.90   \n",
       "\n",
       "      Soil_fertility Soil_type        pH  Pollution_level  Plot_size  \\\n",
       "0               0.62     Sandy  6.169393     8.526684e-02        1.3   \n",
       "1               0.64  Volcanic  5.676648     3.996838e-01        2.2   \n",
       "2               0.69  Volcanic  5.331993     3.580286e-01        3.4   \n",
       "3               0.54     Loamy  5.328150     2.866871e-01        2.4   \n",
       "4               0.72     Sandy  5.721234     4.319027e-02        1.5   \n",
       "...              ...       ...       ...              ...        ...   \n",
       "5649            0.61     Sandy  5.741063     3.286828e-01        1.1   \n",
       "5650            0.54     Sandy  5.445833     1.602583e-01        8.7   \n",
       "5651            0.64  Volcanic  5.385873     8.221326e-09        2.1   \n",
       "5652            0.63      Silt  5.562508     6.917245e-10        1.3   \n",
       "5653            0.64     Sandy  5.087792     2.612715e-01        0.5   \n",
       "\n",
       "      Crop_type Annual_yield  Standard_yield  \n",
       "0      0.751354      cassava        0.577964  \n",
       "1      1.069865      cassava        0.486302  \n",
       "2      2.208801          tea        0.649647  \n",
       "3      1.277635      cassava        0.532348  \n",
       "4      0.832614        wheat        0.555076  \n",
       "...         ...          ...             ...  \n",
       "5649   0.609930       potato        0.554482  \n",
       "5650   3.812289        maize        0.438194  \n",
       "5651   1.681629          tea        0.800776  \n",
       "5652   0.659874      cassava        0.507595  \n",
       "5653   0.226532        wheat        0.453064  \n",
       "\n",
       "[5654 rows x 18 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQL_engine = create_db_engine('sqlite:///Maji_Ndogo_farm_survey_small.db')\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df = query_data(SQL_engine, sql_query)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef469416",
   "metadata": {},
   "source": [
    " `create_db_engine()` function inside the `query_data()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66b65d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field_ID</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Location</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Min_temperature_C</th>\n",
       "      <th>Max_temperature_C</th>\n",
       "      <th>Ave_temps</th>\n",
       "      <th>Soil_fertility</th>\n",
       "      <th>Soil_type</th>\n",
       "      <th>pH</th>\n",
       "      <th>Pollution_level</th>\n",
       "      <th>Plot_size</th>\n",
       "      <th>Crop_type</th>\n",
       "      <th>Annual_yield</th>\n",
       "      <th>Standard_yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40734</td>\n",
       "      <td>786.05580</td>\n",
       "      <td>-7.389911</td>\n",
       "      <td>-7.556202</td>\n",
       "      <td>Rural_Akatsi</td>\n",
       "      <td>14.795113</td>\n",
       "      <td>1125.2</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>33.1</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>6.169393</td>\n",
       "      <td>8.526684e-02</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.751354</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.577964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30629</td>\n",
       "      <td>674.33410</td>\n",
       "      <td>-7.736849</td>\n",
       "      <td>-1.051539</td>\n",
       "      <td>Rural_Sokoto</td>\n",
       "      <td>11.374611</td>\n",
       "      <td>1450.7</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>30.6</td>\n",
       "      <td>13.35</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.676648</td>\n",
       "      <td>3.996838e-01</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.069865</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.486302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39924</td>\n",
       "      <td>826.53390</td>\n",
       "      <td>-9.926616</td>\n",
       "      <td>0.115156</td>\n",
       "      <td>Rural_Sokoto</td>\n",
       "      <td>11.339692</td>\n",
       "      <td>2208.9</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>28.4</td>\n",
       "      <td>13.30</td>\n",
       "      <td>0.69</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.331993</td>\n",
       "      <td>3.580286e-01</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.208801</td>\n",
       "      <td>tea</td>\n",
       "      <td>0.649647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5754</td>\n",
       "      <td>574.94617</td>\n",
       "      <td>-2.420131</td>\n",
       "      <td>-6.592215</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>7.109855</td>\n",
       "      <td>328.8</td>\n",
       "      <td>-5.8</td>\n",
       "      <td>32.2</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.54</td>\n",
       "      <td>Loamy</td>\n",
       "      <td>5.328150</td>\n",
       "      <td>2.866871e-01</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.277635</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.532348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14146</td>\n",
       "      <td>886.35300</td>\n",
       "      <td>-3.055434</td>\n",
       "      <td>-7.952609</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>55.007656</td>\n",
       "      <td>785.2</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>14.25</td>\n",
       "      <td>0.72</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.721234</td>\n",
       "      <td>4.319027e-02</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.832614</td>\n",
       "      <td>wheat</td>\n",
       "      <td>0.555076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5649</th>\n",
       "      <td>11472</td>\n",
       "      <td>681.36145</td>\n",
       "      <td>-7.358371</td>\n",
       "      <td>-6.254369</td>\n",
       "      <td>Rural_Akatsi</td>\n",
       "      <td>16.213196</td>\n",
       "      <td>885.7</td>\n",
       "      <td>-4.3</td>\n",
       "      <td>33.4</td>\n",
       "      <td>14.55</td>\n",
       "      <td>0.61</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.741063</td>\n",
       "      <td>3.286828e-01</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.609930</td>\n",
       "      <td>potato</td>\n",
       "      <td>0.554482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>19660</td>\n",
       "      <td>667.02120</td>\n",
       "      <td>-3.154559</td>\n",
       "      <td>-4.475046</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>2.397553</td>\n",
       "      <td>501.1</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>32.1</td>\n",
       "      <td>13.65</td>\n",
       "      <td>0.54</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.445833</td>\n",
       "      <td>1.602583e-01</td>\n",
       "      <td>8.7</td>\n",
       "      <td>3.812289</td>\n",
       "      <td>maize</td>\n",
       "      <td>0.438194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>41296</td>\n",
       "      <td>670.77900</td>\n",
       "      <td>-14.472861</td>\n",
       "      <td>-6.110221</td>\n",
       "      <td>Rural_Hawassa</td>\n",
       "      <td>7.636470</td>\n",
       "      <td>1586.6</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>33.4</td>\n",
       "      <td>14.80</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.385873</td>\n",
       "      <td>8.221326e-09</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.681629</td>\n",
       "      <td>tea</td>\n",
       "      <td>0.800776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>33090</td>\n",
       "      <td>429.48840</td>\n",
       "      <td>-14.653089</td>\n",
       "      <td>-6.984116</td>\n",
       "      <td>Rural_Hawassa</td>\n",
       "      <td>13.944720</td>\n",
       "      <td>1272.2</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>34.6</td>\n",
       "      <td>14.20</td>\n",
       "      <td>0.63</td>\n",
       "      <td>Silt</td>\n",
       "      <td>5.562508</td>\n",
       "      <td>6.917245e-10</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.659874</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.507595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>8375</td>\n",
       "      <td>763.09030</td>\n",
       "      <td>-4.317028</td>\n",
       "      <td>-6.344461</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>35.189430</td>\n",
       "      <td>516.4</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>29.6</td>\n",
       "      <td>12.90</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.087792</td>\n",
       "      <td>2.612715e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.226532</td>\n",
       "      <td>wheat</td>\n",
       "      <td>0.453064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5654 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Field_ID  Elevation   Latitude  Longitude        Location      Slope  \\\n",
       "0        40734  786.05580  -7.389911  -7.556202    Rural_Akatsi  14.795113   \n",
       "1        30629  674.33410  -7.736849  -1.051539    Rural_Sokoto  11.374611   \n",
       "2        39924  826.53390  -9.926616   0.115156    Rural_Sokoto  11.339692   \n",
       "3         5754  574.94617  -2.420131  -6.592215  Rural_Kilimani   7.109855   \n",
       "4        14146  886.35300  -3.055434  -7.952609  Rural_Kilimani  55.007656   \n",
       "...        ...        ...        ...        ...             ...        ...   \n",
       "5649     11472  681.36145  -7.358371  -6.254369    Rural_Akatsi  16.213196   \n",
       "5650     19660  667.02120  -3.154559  -4.475046  Rural_Kilimani   2.397553   \n",
       "5651     41296  670.77900 -14.472861  -6.110221   Rural_Hawassa   7.636470   \n",
       "5652     33090  429.48840 -14.653089  -6.984116   Rural_Hawassa  13.944720   \n",
       "5653      8375  763.09030  -4.317028  -6.344461  Rural_Kilimani  35.189430   \n",
       "\n",
       "      Rainfall  Min_temperature_C  Max_temperature_C  Ave_temps  \\\n",
       "0       1125.2               -3.1               33.1      15.00   \n",
       "1       1450.7               -3.9               30.6      13.35   \n",
       "2       2208.9               -1.8               28.4      13.30   \n",
       "3        328.8               -5.8               32.2      13.20   \n",
       "4        785.2               -2.5               31.0      14.25   \n",
       "...        ...                ...                ...        ...   \n",
       "5649     885.7               -4.3               33.4      14.55   \n",
       "5650     501.1               -4.8               32.1      13.65   \n",
       "5651    1586.6               -3.8               33.4      14.80   \n",
       "5652    1272.2               -6.2               34.6      14.20   \n",
       "5653     516.4               -3.8               29.6      12.90   \n",
       "\n",
       "      Soil_fertility Soil_type        pH  Pollution_level  Plot_size  \\\n",
       "0               0.62     Sandy  6.169393     8.526684e-02        1.3   \n",
       "1               0.64  Volcanic  5.676648     3.996838e-01        2.2   \n",
       "2               0.69  Volcanic  5.331993     3.580286e-01        3.4   \n",
       "3               0.54     Loamy  5.328150     2.866871e-01        2.4   \n",
       "4               0.72     Sandy  5.721234     4.319027e-02        1.5   \n",
       "...              ...       ...       ...              ...        ...   \n",
       "5649            0.61     Sandy  5.741063     3.286828e-01        1.1   \n",
       "5650            0.54     Sandy  5.445833     1.602583e-01        8.7   \n",
       "5651            0.64  Volcanic  5.385873     8.221326e-09        2.1   \n",
       "5652            0.63      Silt  5.562508     6.917245e-10        1.3   \n",
       "5653            0.64     Sandy  5.087792     2.612715e-01        0.5   \n",
       "\n",
       "      Crop_type Annual_yield  Standard_yield  \n",
       "0      0.751354      cassava        0.577964  \n",
       "1      1.069865      cassava        0.486302  \n",
       "2      2.208801          tea        0.649647  \n",
       "3      1.277635      cassava        0.532348  \n",
       "4      0.832614        wheat        0.555076  \n",
       "...         ...          ...             ...  \n",
       "5649   0.609930       potato        0.554482  \n",
       "5650   3.812289        maize        0.438194  \n",
       "5651   1.681629          tea        0.800776  \n",
       "5652   0.659874      cassava        0.507595  \n",
       "5653   0.226532        wheat        0.453064  \n",
       "\n",
       "[5654 rows x 18 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df = query_data(create_db_engine('sqlite:///Maji_Ndogo_farm_survey_small.db'), sql_query)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b23b3-32df-46eb-92ef-c4bb6e5372d6",
   "metadata": {},
   "source": [
    "### below cell return an empty df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "892fbeaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field_ID</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Location</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Min_temperature_C</th>\n",
       "      <th>Max_temperature_C</th>\n",
       "      <th>Ave_temps</th>\n",
       "      <th>Soil_fertility</th>\n",
       "      <th>Soil_type</th>\n",
       "      <th>pH</th>\n",
       "      <th>Pollution_level</th>\n",
       "      <th>Plot_size</th>\n",
       "      <th>Crop_type</th>\n",
       "      <th>Annual_yield</th>\n",
       "      <th>Standard_yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Field_ID, Elevation, Latitude, Longitude, Location, Slope, Rainfall, Min_temperature_C, Max_temperature_C, Ave_temps, Soil_fertility, Soil_type, pH, Pollution_level, Plot_size, Crop_type, Annual_yield, Standard_yield]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQL_engine = create_db_engine('sqlite:///Maji_Ndogo_farm_survey_small.db')\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "WHERE Rainfall < 0 \n",
    "\"\"\"\n",
    "# The last line won't ever be true, so no results will be returned. \n",
    "\n",
    "df = query_data(SQL_engine, sql_query)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89985eb4",
   "metadata": {},
   "source": [
    "The empty DataFrame, its is because SQL returned an empty query result. if one try to filter results, we get an answer that would not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea68a976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Rainfall, dtype: bool)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Rainfall'] > 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabaefaf",
   "metadata": {},
   "source": [
    "to avoid this one need to add error handling into our code so that we stop the process if something is wrong, and tell us what the problem is before we continue. \n",
    "\n",
    "Secondly, to help us understand how our code is executing we're going to add some logs. While print statements can help us to debug our code,one  have to remove them once our code goes into use, one by one. `logging` is a better way to debug our code than print statements because one can add `logging.INFO()` logs to know what our code is doing, and `logging.DEBUG()` statements that have more detail in case we want to debug a specific loop in a bit more in detail. There are also various other tools to use, and we can also silence all logging with a single line of code. If we used print statements, we will have to comment them out one by one. \n",
    "\n",
    " apply these two ideas, we get the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12778c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Name our logger so we know that logs from this module come from the data_ingestion module\n",
    "logger = logging.getLogger('data_ingestion')\n",
    "\n",
    "# Set a basic logging message up that prints out a timestamp, the name of our logger, and the message\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def create_db_engine(db_path):\n",
    "    try:\n",
    "        engine = create_engine(db_path)\n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            pass\n",
    "        # test if the database engine was created successfully\n",
    "        logger.info(\"Database engine created successfully.\")\n",
    "        return engine # Return the engine object if it all works well\n",
    "    except ImportError: #If we get an ImportError, inform the user SQLAlchemy is not installed\n",
    "        logger.error(\"SQLAlchemy is required to use this function. Please install it first.\")\n",
    "        raise e\n",
    "    except Exception as e:# If we fail to create an engine inform the user\n",
    "        logger.error(f\"Failed to create database engine. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def query_data(engine, sql_query):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            df = pd.read_sql_query(text(sql_query), connection)\n",
    "        if df.empty:\n",
    "            # Log a message or handle the empty DataFrame scenario as needed\n",
    "            msg = \"The query returned an empty DataFrame.\"\n",
    "            logger.error(msg)\n",
    "            raise ValueError(msg)\n",
    "        logger.info(\"Query executed successfully.\")\n",
    "        return df\n",
    "    except ValueError as e: \n",
    "        logger.error(f\"SQL query failed. Error: {e}\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while querying the database. Error: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b8bb7d",
   "metadata": {},
   "source": [
    "This code has an error as processing was prevented further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cc433f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 12:15:11,517 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 12:15:11,528 - data_ingestion - ERROR - The query returned an empty DataFrame.\n",
      "2024-02-26 12:15:11,529 - data_ingestion - ERROR - SQL query failed. Error: The query returned an empty DataFrame.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The query returned an empty DataFrame.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 13\u001b[0m\n\u001b[0;32m      3\u001b[0m sql_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124mSELECT *\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mFROM geographic_features\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124mWHERE Rainfall < 0 \u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# The last line won't ever be true, so no results will be returned. \u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mquery_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSQL_engine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m df\n",
      "Cell \u001b[1;32mIn[15], line 40\u001b[0m, in \u001b[0;36mquery_data\u001b[1;34m(engine, sql_query)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[0;32m     39\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQL query failed. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     42\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while querying the database. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 35\u001b[0m, in \u001b[0;36mquery_data\u001b[1;34m(engine, sql_query)\u001b[0m\n\u001b[0;32m     33\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe query returned an empty DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(msg)\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m     36\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery executed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[1;31mValueError\u001b[0m: The query returned an empty DataFrame."
     ]
    }
   ],
   "source": [
    "SQL_engine = create_db_engine('sqlite:///Maji_Ndogo_farm_survey_small.db')\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "WHERE Rainfall < 0 \n",
    "\"\"\"\n",
    "# The last line won't ever be true, so no results will be returned. \n",
    "\n",
    "df = query_data(SQL_engine, sql_query)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d867e110",
   "metadata": {},
   "source": [
    " including the CSV data handling. This is the original code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31b5eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\")\n",
    "weather_station_mapping_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae0f7f6",
   "metadata": {},
   "source": [
    "These two files are imported in the same way, one function can do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8c8a6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 10:44:43,785 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-05-04 10:44:45,191 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    }
   ],
   "source": [
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "\n",
    "def read_from_web_CSV(URL):\n",
    "    try:\n",
    "        df = pd.read_csv(URL)\n",
    "        logger.info(\"CSV file read successfully from the web.\")\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        logger.error(\"The URL does not point to a valid CSV file. Please check the URL and try again.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read CSV from the web. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    \n",
    "weather_df = read_from_web_CSV(weather_data_URL)\n",
    "weather_mapping_data = read_from_web_CSV(weather_mapping_data_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b77cfc",
   "metadata": {},
   "source": [
    "The code can connect to a database for the field data, use a query to retrieve data, and create a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0462cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Name our logger so we know that logs from this module come from the data_ingestion module\n",
    "logger = logging.getLogger('data_ingestion')\n",
    "# Set a basic logging message up that prints out a timestamp, the name of our logger, and the message\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "def create_db_engine(db_path):\n",
    "    try:\n",
    "        engine = create_engine(db_path)\n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            pass\n",
    "        # test if the database engine was created successfully\n",
    "        logger.info(\"Database engine created successfully.\")\n",
    "        return engine # Return the engine object if it all works well\n",
    "    except ImportError: #If we get an ImportError, inform the user SQLAlchemy is not installed\n",
    "        logger.error(\"SQLAlchemy is required to use this function. Please install it first.\")\n",
    "        raise e\n",
    "    except Exception as e:# If we fail to create an engine inform the user\n",
    "        logger.error(f\"Failed to create database engine. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def query_data(engine, sql_query):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            df = pd.read_sql_query(text(sql_query), connection)\n",
    "        if df.empty:\n",
    "            # Log a message or handle the empty DataFrame scenario as needed\n",
    "            msg = \"The query returned an empty DataFrame.\"\n",
    "            logger.error(msg)\n",
    "            raise ValueError(msg)\n",
    "        logger.info(\"Query executed successfully.\")\n",
    "        return df\n",
    "    except ValueError as e: \n",
    "        logger.error(f\"SQL query failed. Error: {e}\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while querying the database. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def read_from_web_CSV(URL):\n",
    "    try:\n",
    "        df = pd.read_csv(URL)\n",
    "        logger.info(\"CSV file read successfully from the web.\")\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        logger.error(\"The URL does not point to a valid CSV file. Please check the URL and try again.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read CSV from the web. Error: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39a85194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 10:47:55,512 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-05-04 10:47:55,785 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-05-04 10:47:57,480 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-05-04 10:47:58,855 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    }
   ],
   "source": [
    "# Testing module functions  \n",
    "field_df = query_data(create_db_engine(db_path), sql_query)   \n",
    "weather_df = read_from_web_CSV(weather_data_URL)\n",
    "weather_mapping_df = read_from_web_CSV(weather_mapping_data_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a713ffaa",
   "metadata": {},
   "source": [
    "\n",
    "Note that there are imports at the top of the cell.it needs to import packages like SQL Alchemy and Pandas. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "977a2cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field_df: (5654, 18), weather_df: (1843, 2), weather_mapping_df: (5654, 3)\n"
     ]
    }
   ],
   "source": [
    "field_test = field_df.shape\n",
    "weather_test = weather_df.shape\n",
    "weather_mapping_test = weather_mapping_df.shape\n",
    "print(f\"field_df: {field_test}, weather_df: {weather_test}, weather_mapping_df: {weather_mapping_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9105b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "Module: Data Ingestion\n",
    "\n",
    "This module handles the ingestion of data into the Maji Ndogo farm survey database\n",
    "\n",
    "Import Statements: The code imports necessary modules such as create_engine and text from SQLAlchemy, logging, and pandas for data manipulation.\n",
    "\n",
    "Logger Configuration: It configures a logger named 'data_ingestion' using the Python logging module. \n",
    "The logger will output log messages of level INFO and above to the console with a specific format including a timestamp, logger name, \n",
    "and log level.\n",
    "\n",
    "Database Path: It defines the path to a SQLite database file named 'Maji_Ndogo_farm_survey_small.db'.\n",
    "\n",
    "\"\"\"\n",
    "# Name our logger so we know that logs from this module come from the data_ingestion module\n",
    "logger = logging.getLogger('data_ingestion')\n",
    "# Set a basic logging message up that prints out a timestamp, the name of our logger, and the message\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dac76c7f-57f1-4f0b-a08c-b8c4ad827c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "from sqlalchemy import create_engine, text\n",
    "import logging\n",
    "import pandas as pd\n",
    "def create_db_engine(db_path):\n",
    "\n",
    "    \"\"\"Create a database engine.\n",
    "\n",
    "    Args:\n",
    "        db_path (str): The path to the database.\n",
    "\n",
    "    Returns:\n",
    "        engine (sqlalchemy.engine.Engine): The SQLAlchemy engine object.\n",
    "\n",
    "    Raises:\n",
    "        ImportError: If SQLAlchemy is not installed.\n",
    "        Exception: If there is an error creating the database engine.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        engine = create_engine(db_path)\n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            pass\n",
    "        # test if the database engine was created successfully\n",
    "        logger.info(\"Database engine created successfully.\")\n",
    "        return engine  # Return the engine object if it all works well\n",
    "    except ImportError:  # If we get an ImportError, inform the user SQLAlchemy is not installed\n",
    "        logger.error(\"SQLAlchemy is required to use this function. Please install it first.\")\n",
    "        raise e\n",
    "    except Exception as e:  # If we fail to create an engine inform the user\n",
    "        logger.error(f\"Failed to create database engine. Error: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def query_data(engine, sql_query):\n",
    "    \"\"\"Query data: executes a SQL query on the database engine and returns the result as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        engine (sqlalchemy.engine.Engine): The SQLAlchemy engine object.\n",
    "        sql_query (str): The SQL query to be executed.\n",
    "\n",
    "    Returns:\n",
    "        df (pandas.DataFrame): The result of the SQL query as a pandas DataFrame.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if the query returns an empty df\n",
    "        Exception: if there is an error while querying the database.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            df = pd.read_sql_query(text(sql_query), connection)\n",
    "        if df.empty:\n",
    "            # Log a message or handle the empty DataFrame scenario as needed\n",
    "            msg = \"The query returned an empty DataFrame.\"\n",
    "            logger.error(msg)\n",
    "            raise ValueError(msg)\n",
    "        logger.info(\"Query executed successfully.\")\n",
    "        return df\n",
    "    except ValueError as e:\n",
    "        logger.error(f\"SQL query failed. Error: {e}\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while querying the database. Error: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "def read_from_web_CSV(URL):\n",
    "    \"\"\"\n",
    "    Read from web csv: reads csv file from the specified URL and returns it as pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        URL (str): URL from where csv file is located.\n",
    "\n",
    "    Returns:\n",
    "        dataframe (pandas.DataFrame): The DataFrame with the csv data.\n",
    "\n",
    "    Raises:\n",
    "        pd.errors.EmptyDataError: if URL is not found or it is empty.\n",
    "        Exception: if the URL is not readable from the web or there is an error while reading the file from the web.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(URL)\n",
    "        logger.info(\"CSV file read successfully from the web.\")\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        logger.error(\"The URL does not point to a valid CSV file. Please check the URL and try again.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read CSV from the web. Error: {e}\")\n",
    "        raise e\n",
    "\n",
    "### END FUNCTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0c4fb",
   "metadata": {},
   "source": [
    "Testing code to make sure it works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27dcc193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 10:51:22,167 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-05-04 10:51:22,704 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-05-04 10:51:24,784 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-05-04 10:51:26,321 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field_df: (5654, 18), weather_df: (1843, 2), weather_mapping_df: (5654, 3)\n"
     ]
    }
   ],
   "source": [
    "# Testing module functions  \n",
    "field_df = query_data(create_db_engine(db_path), sql_query)   \n",
    "weather_df = read_from_web_CSV(weather_data_URL)\n",
    "weather_mapping_df = read_from_web_CSV(weather_mapping_data_URL)\n",
    "\n",
    "field_test = field_df.shape\n",
    "weather_test = weather_df.shape\n",
    "weather_mapping_test = weather_mapping_df.shape\n",
    "print(f\"field_df: {field_test}, weather_df: {weather_test}, weather_mapping_df: {weather_mapping_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6c609b",
   "metadata": {},
   "source": [
    "creating a new file, `data_ingestion.py` and import the functions into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab292cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_ingestion\n",
      "data_ingestion\n",
      "data_ingestion\n"
     ]
    }
   ],
   "source": [
    "# Importing our new module\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "\n",
    "#Checking if the function names are now associated with the module\n",
    "print(create_db_engine.__module__)\n",
    "print(query_data.__module__)\n",
    "print(read_from_web_CSV.__module__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2fb109",
   "metadata": {},
   "source": [
    "\n",
    "names `create_db_engine`, `query_data`, `read_from_web_CSV` are linked to the `data_ingestion` module, so the  module is imported correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87170695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 10:52:49,118 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-05-04 10:52:49,349 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-05-04 10:52:51,034 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-05-04 10:52:52,578 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field_df: (5654, 18), weather_df: (1843, 2), weather_mapping_df: (5654, 3)\n"
     ]
    }
   ],
   "source": [
    "field_df = query_data(create_db_engine(db_path), sql_query)   \n",
    "weather_df = read_from_web_CSV(weather_data_URL)\n",
    "weather_mapping_df = read_from_web_CSV(weather_mapping_data_URL)\n",
    "\n",
    "field_test = field_df.shape\n",
    "weather_test = weather_df.shape\n",
    "weather_mapping_test = weather_mapping_df.shape\n",
    "print(f\"field_df: {field_test}, weather_df: {weather_test}, weather_mapping_df: {weather_mapping_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797db70",
   "metadata": {},
   "source": [
    "## Field data processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b43cc5",
   "metadata": {},
   "source": [
    "processing the field data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b41eb954",
   "metadata": {},
   "outputs": [],
   "source": [
    "MD_agric_df = field_df.copy()\n",
    "\n",
    "MD_agric_df.rename(columns={'Annual_yield': 'Crop_type_Temp', 'Crop_type': 'Annual_yield'}, inplace=True)\n",
    "MD_agric_df.rename(columns={'Crop_type_Temp': 'Crop_type'}, inplace=True)\n",
    "MD_agric_df['Elevation'] = MD_agric_df['Elevation'].abs()\n",
    "\n",
    "# Correcting 'Crop_type' column\n",
    "def correct_crop_type(crop):\n",
    "    corrections = {\n",
    "        'cassaval': 'cassava',\n",
    "        'wheatn': 'wheat',\n",
    "        'teaa': 'tea'\n",
    "    }\n",
    "    return corrections.get(crop, crop)  # Get the corrected crop type, or return the original if not in corrections\n",
    "\n",
    "# Apply the correction function to the Crop_type column\n",
    "MD_agric_df['Crop_type'] = MD_agric_df['Crop_type'].apply(correct_crop_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e25a2",
   "metadata": {},
   "source": [
    "building a Class that encapsulates the whole data processing process for the field-related data called `FieldDataProcessor`. In the class, it will create a DataFrame attribute and methods that alter that attribute.  encapsulate all of the logic in this `FieldDataProcessor` class,  abstract all of the details and only need to call something like `FieldDataProcessor.process_data()`. \n",
    "\n",
    "create the class framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d4179f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "class FieldDataProcessor:\n",
    "\n",
    "    def __init__(self, logging_level=\"INFO\"): # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"\n",
    "            SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "        self.initialize_logging(logging_level)\n",
    "        \n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class. \n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # DataFrame methods \n",
    "    def ingest_sql_data(self):\n",
    "        # First we want to get the data from the SQL database\n",
    "        pass\n",
    "    \n",
    "    def rename_columns(self):\n",
    "        # Annual_yield and Crop_type must be swapped\n",
    "        pass\n",
    "\n",
    "    def apply_corrections(self):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods and applies the changes, step by step. This is the method we will call, and it will call the other methods in order\n",
    "        \n",
    "        weather_map_df = self.weather_station_mapping() \n",
    "        self.df = self.ingest_sql_data()\n",
    "        self.df = self.rename_columns()\n",
    "        self.df = self.apply_corrections()\n",
    "        self.df = self.df.merge(weather_map_df, on='Field_ID', how='left')\n",
    "        self.df = self.df.drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10660c3f",
   "metadata": {},
   "source": [
    "instantiating the class, and call one method, `.process()` to ingest and clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f30d2d73",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'merge'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This code won't run for now, since we have not defined all of the methods.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m field_processor \u001b[38;5;241m=\u001b[39m FieldDataProcessor()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mfield_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 84\u001b[0m, in \u001b[0;36mFieldDataProcessor.process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrename_columns()\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_corrections()\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m(weather_map_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mField_ID\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'merge'"
     ]
    }
   ],
   "source": [
    "# This code won't run for now, since we have not defined all of the methods.\n",
    "field_processor = FieldDataProcessor()\n",
    "field_processor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe519e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_df = field_processor.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfabeb1",
   "metadata": {},
   "source": [
    "### `def ingest_sql_data()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b1aae",
   "metadata": {},
   "source": [
    "create a copy of the class, and start filling out the code for the methods.  dropping the `.process()` method for now and  add it back once it all works. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108198f",
   "metadata": {},
   "source": [
    " Unscramble the code in the `.ingest_sql_data()` method. The method should return the initial DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6426c663-58c9-48f3-9570-cc693eb8e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "class FieldDataProcessor:\n",
    "\n",
    "    def __init__(self, logging_level=\"INFO\"): # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "        \n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    def ingest_sql_data(self):\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        return self.df\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "       \n",
    "        \n",
    "\n",
    "    def rename_columns(self):\n",
    "        # Annual_yield and Crop_type must be swapped\n",
    "        pass\n",
    "\n",
    "    def apply_corrections(self):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6352f",
   "metadata": {},
   "source": [
    "We can use the code below to check if the code works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b3f776b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 11:02:16,233 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-05-04 11:02:17,508 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-05-04 11:02:17,510 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 18)\n"
     ]
    }
   ],
   "source": [
    "field_processor = FieldDataProcessor()\n",
    "field_processor.ingest_sql_data()\n",
    "field_df = field_processor.df\n",
    "print(field_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7676579c",
   "metadata": {},
   "source": [
    "### `def rename_columns()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c11f136",
   "metadata": {},
   "source": [
    "adding `rename_columns()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47548fb7",
   "metadata": {},
   "source": [
    " Copying the class into the top of this cell, andunscramblinge the code sections in the `.rename_columns()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "369c190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy in your class including the ingest_sql_data method here\n",
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "class FieldDataProcessor:\n",
    "\n",
    "    def __init__(self, logging_level=\"INFO\"): # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "        \n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    def ingest_sql_data(self):\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        return self.df\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "       \n",
    "        \n",
    "\n",
    "    def rename_columns(self):\n",
    "        # Annual_yield and Crop_type must be swapped\n",
    "        pass\n",
    "\n",
    "    def apply_corrections(self):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass\n",
    "    def rename_columns(self):\n",
    "        # Extract the columns to rename from the configuration\n",
    "        column1, column2 = list(self.columns_to_rename.keys())[0], list(self.columns_to_rename.values())[0]   \n",
    "          # Temporarily rename one of the columns to avoid a naming conflict\n",
    "        temp_name = \"__temp_name_for_swap__\"\n",
    "        while temp_name in self.df.columns:\n",
    "            temp_name += \"_\"\n",
    "        self.logger.info(f\"Swapped columns: {column1} with {column2}\")\n",
    "\n",
    "        # Perform the swap\n",
    "        self.df = self.df.rename(columns={column1: temp_name, column2: column1})\n",
    "        self.df = self.df.rename(columns={temp_name: column2})\n",
    "           \n",
    "\n",
    "      \n",
    "            \n",
    "    def apply_corrections(self):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28acacaf",
   "metadata": {},
   "source": [
    "`rename_columns()` does not return anything. It doesn't need to, because it is modifying the class attribute (data) `self.df`. This is the benefit of using a class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de6c48",
   "metadata": {},
   "source": [
    "This code  instantiate the class, connect to the database, and swap the column names.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7233b3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 11:04:38,751 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-05-04 11:04:39,230 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-05-04 11:04:39,233 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-05-04 11:04:39,264 - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.751354\n",
       "1    1.069865\n",
       "2    2.208801\n",
       "Name: Annual_yield, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_processor = FieldDataProcessor()\n",
    "field_processor.ingest_sql_data()\n",
    "field_processor.rename_columns()\n",
    "field_df = field_processor.df\n",
    "field_df['Annual_yield'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e74278",
   "metadata": {},
   "source": [
    "### `def apply_corrections()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5dafe",
   "metadata": {},
   "source": [
    " Copying  class into the top of this cell in the `.apply_corrections()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3065e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "class FieldDataProcessor:\n",
    "\n",
    "    def __init__(self, logging_level=\"INFO\"): # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "        \n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    def ingest_sql_data(self):\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        return self.df\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "       \n",
    "        \n",
    "\n",
    "    def rename_columns(self):\n",
    "        # Annual_yield and Crop_type must be swapped\n",
    "        pass\n",
    "\n",
    "    def apply_corrections(self):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass\n",
    "# Copy in your class including the ingest_sql_data and  method here\n",
    "    \n",
    "    def apply_corrections(self, column_name='Crop_type', abs_column='Elevation'):\n",
    "        self.df[abs_column] = self.df[abs_column].abs()\n",
    "        self.df[column_name] = self.df[column_name].apply(lambda crop: self.values_to_rename.get(crop, crop))\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a92acc4",
   "metadata": {},
   "source": [
    " testing if  new method works before we move onto the next one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f218e5-a3c8-44a8-9010-408b5c41cd82",
   "metadata": {},
   "source": [
    "field_processor = FieldDataProcessor()\n",
    "field_processor.ingest_sql_data()\n",
    "field_processor.rename_columns()\n",
    "field_processor.apply_corrections()\n",
    "\n",
    "field_df = field_processor.df\n",
    "field_df.query(\"Crop_type in ['cassaval','wheatn']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267cefd8-82b8-4cab-bb2b-8cde6028b386",
   "metadata": {},
   "source": [
    "expected empty data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9794ee01",
   "metadata": {},
   "source": [
    "### `def weather_station_mapping()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29c58353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "class FieldDataProcessor:\n",
    "\n",
    "    def __init__(self, logging_level=\"INFO\"): # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "        \n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    def ingest_sql_data(self):\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        return self.df\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "       \n",
    "        \n",
    "\n",
    "    def rename_columns(self):\n",
    "        # Annual_yield and Crop_type must be swapped\n",
    "        pass\n",
    "\n",
    "    def apply_corrections(self):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass\n",
    "# Copy in your class including the ingest_sql_data and method here\n",
    "\n",
    "\n",
    "        \n",
    "    def weather_station_mapping(self):\n",
    "        weather_map_df = read_from_web_CSV(self.weather_map_data)\n",
    "        self.df = self.df.merge(weather_map_df, on = 'Field_ID', how = 'left')\n",
    "        return read_from_web_CSV(self.weather_map_data)\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods and applies the changes, step by step. This is the method we will call, and it will call the other methods in order.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458198a",
   "metadata": {},
   "source": [
    "Once again, test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c05b721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 11:07:05,510 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-05-04 11:07:05,820 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-05-04 11:07:05,821 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-05-04 11:07:23,182 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-05-04 11:07:30,823 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_processor = FieldDataProcessor()\n",
    "field_processor.ingest_sql_data()\n",
    "field_processor.rename_columns()\n",
    "field_processor.apply_corrections()\n",
    "field_processor.weather_station_mapping()\n",
    "\n",
    "field_df = field_processor.df\n",
    "field_df['Weather_station'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552da32c",
   "metadata": {},
   "source": [
    "### `def process()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37328e56",
   "metadata": {},
   "source": [
    " put it all together. Remembering that the `.process()` method calls all of the other methods in order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bdfc50",
   "metadata": {},
   "source": [
    "completing the `.process()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f20de13-5caf-44f2-99b6-3119e5fcef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy in your class including the ingest_sql_data and  method here\n",
    "# Copy in your class including the ingest_sql_data and  method here\n",
    "\n",
    "\n",
    "class FieldDataProcessor:\n",
    "    import pandas as pd\n",
    "    from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "    import logging\n",
    "\n",
    "    def __init__(self, logging_level=\"INFO\"): # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "        \n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    def ingest_sql_data(self):\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        return self.df\n",
    "       \n",
    "       \n",
    "\n",
    "    def rename_columns(self):\n",
    "        # Annual_yield and Crop_type must be swapped\n",
    "        pass\n",
    "\n",
    "    def apply_corrections(self):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def rename_columns(self):\n",
    "        # Extract the columns to rename from the configuration\n",
    "        column1, column2 = list(self.columns_to_rename.keys())[0], list(self.columns_to_rename.values())[0]\n",
    "        # Temporarily rename one of the columns to avoid a naming conflict\n",
    "        temp_name = \"__temp_name_for_swap__\"\n",
    "        while temp_name in self.df.columns:\n",
    "            temp_name += \"_\"\n",
    "        # Perform the swap\n",
    "        self.df = self.df.rename(columns={column1: temp_name, column2: column1})\n",
    "        self.df = self.df.rename(columns={temp_name: column2})   \n",
    "        \n",
    "        self.logger.info(f\"Swapped columns: {column1} with {column2}\")\n",
    "\n",
    "                    \n",
    "    def apply_corrections(self):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass\n",
    "    \n",
    "    def apply_corrections(self, column_name='Crop_type', abs_column='Elevation'):\n",
    "        self.df[abs_column] = self.df[abs_column].abs()\n",
    "        self.df[column_name] = self.df[column_name].apply(lambda crop: self.values_to_rename.get(crop, crop))\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        weather_map_df = read_from_web_CSV(self.weather_map_data)\n",
    "        self.df = self.df.merge(  weather_map_df, on = \"Field_ID\", how=\"left\")\n",
    "        return weather_map_df\n",
    "    \n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods and applies the changes, step by step. This is the method we will call, and it will call the other methods in order.\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        self.ingest_sql_data()\n",
    "        self.rename_columns()\n",
    "        self.apply_corrections()\n",
    "        weather_station_df = self.weather_station_mapping()\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3645f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 11:08:24,755 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-05-04 11:08:25,251 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-05-04 11:08:25,252 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-05-04 11:08:25,260 - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-05-04 11:08:28,154 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_processor = FieldDataProcessor()\n",
    "field_processor.process()\n",
    "\n",
    "field_df = field_processor.df\n",
    "field_df['Weather_station'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70083d45",
   "metadata": {},
   "source": [
    "### Centralising the data pipeline configuration details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5380c7",
   "metadata": {},
   "source": [
    "some data about the SQL database and web files in the `data_ingestion.py` module, and when we load the `field_data_proccessor.py` module we are referencing it again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b48ddfe6-4fa7-42d8-a325-85d09bb1d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the data_ingestion.py module\n",
    "\n",
    "db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "\n",
    "# From the field_data_processor class\n",
    "class FieldDataProcessor:\n",
    "    def __init__(self):\n",
    "        self.db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf3a72",
   "metadata": {},
   "source": [
    "A good approach is to create a dictionary in our main script that has all of the parameters, and then  reference it in our modules. \n",
    "\n",
    "Adding  the configuration details from the `data_ingestion.py` module into the `config_params` dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57ec9640",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_params = {\n",
    "    \"sql_query\": \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "        \n",
    "            \"\"\", # Insert your SQL query\n",
    "    \"db_path\":'sqlite:///Maji_Ndogo_farm_survey_small.db' , # Insert the db_path of the database\n",
    "    \"columns_to_rename\":{'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}, # Insert the disctionary of columns we want to swop the names of, \n",
    "    \"values_to_rename\":{'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'} , # Insert the croptype renaming dictionary\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\" , # Insert the weather data CSV here\n",
    "    \"weather_mapping_csv\":\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\", # Insert the weather data mapping CSV here\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b794548",
   "metadata": {},
   "source": [
    " removing these lines form the `data_ingestion.py` module file, since one call them from the `FieldDataProcessor` class.\n",
    "\n",
    "Remove the following lines from the `data_ingestion.py` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9c0922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove these lines from the data_ingestion.py module\n",
    "db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5982650b",
   "metadata": {},
   "source": [
    "In the `FieldDataProcessor` class, instead of passing in the parameters as strings,  reference the `config_params` dictionary instead.\n",
    "\n",
    " Altering  the attributes of the `FieldDataProcessor` class to reference the `config_params` dictionary instead. Add `config_params` as a parameter to the class instantiation method as shown below and complete the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5c62c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FieldDataProcessor:\n",
    "\n",
    "    def __init__(self, config_params, logging_level=\"INFO\"):  # Make sure to add this line, passing in config_params to the class \n",
    "        self.db_path = config_params['db_path']\n",
    "        self.sql_query = config_params[\"sql_query\"]\n",
    "        self.columns_to_rename = config_params[\"columns_to_rename\"]\n",
    "        self.values_to_rename = config_params[\"values_to_rename\"]\n",
    "        self.weather_map_data = config_params[\"weather_mapping_csv\"]\n",
    "\n",
    "        # Add the rest of your class code here\n",
    "# Copy in your class including the ingest_sql_data and  method here\n",
    "        \n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    def ingest_sql_data(self):\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        return self.df\n",
    "       \n",
    "       \n",
    "\n",
    "    def rename_columns(self):\n",
    "        # Annual_yield and Crop_type must be swapped\n",
    "        pass\n",
    "\n",
    "    def apply_corrections(self):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def rename_columns(self):\n",
    "        # Extract the columns to rename from the configuration\n",
    "        column1, column2 = list(self.columns_to_rename.keys())[0], list(self.columns_to_rename.values())[0]\n",
    "        # Temporarily rename one of the columns to avoid a naming conflict\n",
    "        temp_name = \"__temp_name_for_swap__\"\n",
    "        while temp_name in self.df.columns:\n",
    "            temp_name += \"_\"\n",
    "        # Perform the swap\n",
    "        self.df = self.df.rename(columns={column1: temp_name, column2: column1})\n",
    "        self.df = self.df.rename(columns={temp_name: column2})   \n",
    "        \n",
    "        self.logger.info(f\"Swapped columns: {column1} with {column2}\")\n",
    "\n",
    "                    \n",
    "    def apply_corrections(self):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass\n",
    "    \n",
    "    def apply_corrections(self, column_name='Crop_type', abs_column='Elevation'):\n",
    "        self.df[abs_column] = self.df[abs_column].abs()\n",
    "        self.df[column_name] = self.df[column_name].apply(lambda crop: self.values_to_rename.get(crop, crop))\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        weather_map_df = read_from_web_CSV(self.weather_map_data)\n",
    "        self.df = self.df.merge(  weather_map_df, on = \"Field_ID\", how=\"left\")\n",
    "        return weather_map_df\n",
    "    \n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods and applies the changes, step by step. This is the method we will call, and it will call the other methods in order.\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        self.ingest_sql_data()\n",
    "        self.rename_columns()\n",
    "        self.apply_corrections()\n",
    "        weather_station_df = self.weather_station_mapping()\n",
    "        \n",
    "              \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae0605f",
   "metadata": {},
   "source": [
    " Instantiate the class with the new dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8712fd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 11:16:13,409 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-05-04 11:16:13,925 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-05-04 11:16:13,926 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-05-04 11:16:13,937 - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-05-04 11:16:15,977 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_params = {\n",
    "    'db_path': 'sqlite:///Maji_Ndogo_farm_survey_small.db',\n",
    "    'sql_query': \"\"\"\n",
    "        SELECT *\n",
    "        FROM geographic_features\n",
    "        LEFT JOIN weather_features USING (Field_ID)\n",
    "        LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "        LEFT JOIN farm_management_features USING (Field_ID)\n",
    "    \"\"\",\n",
    "    'columns_to_rename': {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'},\n",
    "    'values_to_rename': {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'},\n",
    "    'weather_mapping_csv': \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "}\n",
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "\n",
    "field_df = field_processor.df\n",
    "field_df['Weather_station'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f595162",
   "metadata": {},
   "source": [
    "\n",
    "### Creating `field_data_processor.py`\n",
    "\n",
    "having a robust data processing class for the field-related data. final step is to create the module file and document the code.\n",
    "\n",
    " Completing the `field_data_processor` module. Including  all of the required content, ensuing e the module is PEP 8 complient, including all imports and parameter definitions, and creating the `field_data_processor.py` module file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1679c0",
   "metadata": {},
   "source": [
    "Restart the kernel before running this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bcad5629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 11:19:49,778 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-05-04 11:19:50,006 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-05-04 11:19:50,009 - field_data_processor.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-05-04 11:19:50,016 - field_data_processor.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-05-04 11:19:51,385 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # Importing all the packages we will use eventually\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from field_data_processor import FieldDataProcessor # Importing our new module\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params = {\n",
    "    'db_path': 'sqlite:///Maji_Ndogo_farm_survey_small.db',\n",
    "    'sql_query': \"\"\"\n",
    "        SELECT *\n",
    "        FROM geographic_features\n",
    "        LEFT JOIN weather_features USING (Field_ID)\n",
    "        LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "        LEFT JOIN farm_management_features USING (Field_ID)\n",
    "    \"\"\",\n",
    "    'columns_to_rename': {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'},\n",
    "    'values_to_rename': {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'},\n",
    "    'weather_mapping_csv': \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "}\n",
    "# Paste in your config_params dictionary here\n",
    "\n",
    "\n",
    "# Instantiating the class with config_params passed to the class as a parameter \n",
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "field_df = field_processor.df\n",
    "\n",
    "# Test\n",
    "field_df['Weather_station'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c04508d",
   "metadata": {},
   "source": [
    "## Weather data processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a595d0",
   "metadata": {},
   "source": [
    " last module. The `WeatherDataProcessor` class will be dealing with all of the weather-related data.  to instantiate the class, then call a `.process()` method to import and clean the data. Here is the code we used last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af63063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_measurement(message):\n",
    "    import re # Importing the regex pattern\n",
    "    import numpy as np\n",
    "    \n",
    "    weather_station_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\")\n",
    "    weather_station_mapping_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\")\n",
    "    \n",
    "    patterns = {\n",
    "        'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "         'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "        'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "        }\n",
    "    \"\"\"\n",
    "    Extracts a numeric measurement value from a given message string.\n",
    "\n",
    "    The function applies regular expressions to identify and extract\n",
    "    numeric values related to different types of measurements such as\n",
    "    Rainfall, Average Temperatures, and Pollution Levels from a text message.\n",
    "    It returns the key of the matching record, and first matching value as a floating-point number.\n",
    "    \n",
    "    Parameters:\n",
    "    message (str): A string message containing the measurement information.\n",
    "\n",
    "    Returns:\n",
    "    float: The extracted numeric value of the measurement if a match is found;\n",
    "           otherwise, None.\n",
    "\n",
    "    The function uses the following patterns for extraction:\n",
    "    - Rainfall: Matches numbers (including decimal) followed by 'mm', optionally spaced.\n",
    "    - Ave_temps: Matches numbers (including decimal) followed by 'C', optionally spaced.\n",
    "    - Pollution_level: Matches numbers (including decimal) following 'Pollution at' or '='.\n",
    "    \n",
    "    Example usage:\n",
    "    extract_measurement(\"【2022-01-04 21:47:48】温度感应: 现在温度是 12.82C.\")\n",
    "    # Returns: 'Temperature', 12.82\n",
    "    \"\"\"\n",
    "    \n",
    "    for key, pattern in patterns.items(): # Loop through all of the patterns and check if it matches the pattern value.\n",
    "        match = re.search(pattern, message)\n",
    "        if match:\n",
    "            # Extract the first group that matches, which should be the measurement value if all previous matches are empty.\n",
    "            # print(match.groups()) # Uncomment this line to help you debug your regex patterns.\n",
    "            return key, float(next((x for x in match.groups() if x is not None)))\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# The function creates a tuple with the measurement type and value into a Pandas Series\n",
    "result = weather_station_df['Message'].apply(extract_measurement)\n",
    "\n",
    "# Create separate columns for 'Measurement' and 'extracted_value' by unpacking the tuple with Lambda functions.\n",
    "weather_station_df['Measurement'] = result.apply(lambda x: x[0])\n",
    "weather_station_df['Value'] = result.apply(lambda x: x[1])\n",
    "\n",
    "# The function creates a tuple with the measurement type and value into a Pandas Series\n",
    "result = weather_station_df['Message'].apply(extract_measurement)\n",
    "\n",
    "# Create separate columns for 'Measurement' and 'extracted_value' by unpacking the tuple with Lambda functions.\n",
    "weather_station_df['Measurement'] = result.apply(lambda x: x[0])\n",
    "weather_station_df['Value'] = result.apply(lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2937ec",
   "metadata": {},
   "source": [
    "Luckily the other team did most of the work this time, so we only have to fill in a couple of details. First off, we need to add more keys to the `config_params` dictionary. Specifically the regex pattern we used to get the messages, and the URL of the weather data.\n",
    "\n",
    "<br>\n",
    "\n",
    "⚙️ **Task:** Complete the values for the new keys in `config_params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fc8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\")\n",
    "weather_station_mapping_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\")\n",
    "\n",
    "patterns = {\n",
    "    'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "     'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "    'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "    }\n",
    "\n",
    "config_params ={\n",
    "    \"sql_query\": \"\"\"\n",
    "            SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "            , # Insert your SQL query\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db', # Insert the db_path of the database\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'},# Insert the disctionary of columns we want to swop the names of, \n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}, # Insert the croptype renaming dictionary\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the weather data CSV here\n",
    "    \"weather_mapping_csv\":\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\", # Insert the weather data mapping CSV here\n",
    "\n",
    "\n",
    "    # Paste in your previous dictionary data in here\n",
    "\n",
    "    # Add two new keys\n",
    "   \"weather_csv_path\":\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\",\n",
    "# Insert the URL for the weather station data\n",
    "    \"regex_patterns\" : {\n",
    "                        'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "                         'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "                        'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'} ,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d1d93",
   "metadata": {},
   "source": [
    "The class fully set up. one has to make sure the formatting is correct, and that the module is documented properly.\n",
    "\n",
    "Complete the `weather_data_processor` module. Include all of the required content, ensure the module is PEP 8 compliant, include all imports and parameter definitions, and create the `weather_data_processor.py` module file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56158be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the imports we're going to use in the weather data processing module\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from data_ingestion import read_from_web_CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230dfb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION \n",
    "class WeatherDataProcessor:\n",
    "    import re\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import logging\n",
    "    from data_ingestion import read_from_web_CSV\n",
    "    def __init__(self, config_params, logging_level=\"INFO\"): # Now we're passing in the confi_params dictionary already\n",
    "        self.weather_station_data = config_params['weather_csv_path']\n",
    "        self.patterns = config_params['regex_patterns']\n",
    "        self.weather_df = None  # Initialize weather_df as None or as an empty DataFrame\n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "    def initialize_logging(self, logging_level):\n",
    "        logger_name = __name__ + \".WeatherDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        self.weather_df = read_from_web_CSV(self.weather_station_data)\n",
    "        self.logger.info(\"Successfully loaded weather station data from the web.\") \n",
    "        # Here, you can apply any initial transformations to self.weather_df if necessary.\n",
    "\n",
    "    \n",
    "    def extract_measurement(self, message):\n",
    "        for key, pattern in self.patterns.items():\n",
    "            match = re.search(pattern, message)\n",
    "            if match:\n",
    "                self.logger.debug(f\"Measurement extracted: {key}\")\n",
    "                return key, float(next((x for x in match.groups() if x is not None)))\n",
    "        self.logger.debug(\"No measurement match found.\")\n",
    "        return None, None\n",
    "\n",
    "    def process_messages(self):\n",
    "        if self.weather_df is not None:\n",
    "            result = self.weather_df['Message'].apply(self.extract_measurement)\n",
    "            self.weather_df['Measurement'], self.weather_df['Value'] = zip(*result)\n",
    "            self.logger.info(\"Messages processed and measurements extracted.\")\n",
    "        else:\n",
    "            self.logger.warning(\"weather_df is not initialized, skipping message processing.\")\n",
    "        return self.weather_df\n",
    "\n",
    "    def calculate_means(self):\n",
    "        if self.weather_df is not None:\n",
    "            means = self.weather_df.groupby(by=['Weather_station_ID', 'Measurement'])['Value'].mean()\n",
    "            self.logger.info(\"Mean values calculated.\")\n",
    "            return means.unstack()\n",
    "        else:\n",
    "            self.logger.warning(\"weather_df is not initialized, cannot calculate means.\")\n",
    "            return None\n",
    "    \n",
    "    def process(self):\n",
    "        self.weather_station_mapping()  # Load and assign data to weather_df\n",
    "        self.process_messages()  # Process messages to extract measurements\n",
    "        self.logger.info(\"Data processing completed.\")\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71179f10",
   "metadata": {},
   "source": [
    "Once we have the `weather_data_processor` module set up, one can run the code below to import the new module, and make sure our module worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af300e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from field_data_processor import FieldDataProcessor\n",
    "from weather_data_processor import WeatherDataProcessor\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params = {\n",
    "    \"sql_query\": \"\"\"\n",
    "            SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "            , # Insert your SQL query\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db', # Insert the db_path of the database\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'},# Insert the disctionary of columns we want to swop the names of, \n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}, # Insert the croptype renaming dictionary\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the weather data CSV here\n",
    "    \"weather_mapping_csv\":\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\", # Insert the weather data mapping CSV here\n",
    "\n",
    "\n",
    "    # Paste in your previous dictionary data in here\n",
    "\n",
    "    # Add two new keys\n",
    "   \"weather_csv_path\":\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\",\n",
    "# Insert the URL for the weather station data\n",
    "    \"regex_patterns\" : {\n",
    "                        'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "                         'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "                        'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'} ,\n",
    "\n",
    "}\n",
    "# Ignoring the field data for now.\n",
    "# field_processor = FieldDataProcessor(config_params)\n",
    "# field_processor.process()\n",
    "# field_df = field_processor.df\n",
    "\n",
    "weather_processor = WeatherDataProcessor(config_params)\n",
    "weather_processor.process()\n",
    "weather_df = weather_processor.weather_df\n",
    "\n",
    "weather_df['Measurement'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d54794",
   "metadata": {},
   "source": [
    "### Validating our data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a9dec5",
   "metadata": {},
   "source": [
    " finally having working modules that now automatically pull data from the database  (or the web), process it, clean it, and return our starting DataFrame.\n",
    " use your `config_params` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0350663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from field_data_processor import FieldDataProcessor\n",
    "from weather_data_processor import WeatherDataProcessor\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params ={\n",
    "    \"sql_query\": \"\"\"\n",
    "            SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "            , # Insert your SQL query\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db', # Insert the db_path of the database\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'},# Insert the disctionary of columns we want to swop the names of, \n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}, # Insert the croptype renaming dictionary\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the weather data CSV here\n",
    "    \"weather_mapping_csv\":\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\", # Insert the weather data mapping CSV here\n",
    "\n",
    "\n",
    "    # Paste in your previous dictionary data in here\n",
    "\n",
    "    # Add two new keys\n",
    "   \"weather_csv_path\":\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\",\n",
    "# Insert the URL for the weather station data\n",
    "    \"regex_patterns\" : {\n",
    "                        'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "                         'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "                        'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'} ,\n",
    "\n",
    "} # Paste in your config_params dictionary here\n",
    "\n",
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "field_df = field_processor.df\n",
    "\n",
    "weather_processor = WeatherDataProcessor(config_params)\n",
    "weather_processor.process()\n",
    "weather_df = weather_processor.weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611d171",
   "metadata": {},
   "source": [
    "There should be a `validate_data.py` file in the notebook directory. This is a `pytest` script that does a couple of tests to see if the data we're expecting, is what we actually have. Have a look at the test script, and try to understand what we're testing.\n",
    "\n",
    "`pytest` normally runs from the command line because it is set up to be automated. To test the data, we have to give `pytest` access to that data. The simplest way to do this is by creating CSV files, importing them into `validate_data.py`, and running the tests.\n",
    "\n",
    "The following code creates CSV files, runs `pytest` in the terminal using `!pytest validate_data.py -v`, and deletes the CSV files once the test is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5456e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytest\n",
    "!pip install pytest\n",
    "weather_df.to_csv('sampled_weather_df.csv', index=False)\n",
    "field_df.to_csv('sampled_field_df.csv', index=False)\n",
    "\n",
    "!pytest validate_data.py -v\n",
    "\n",
    "import os# Define the file paths\n",
    "weather_csv_path = 'sampled_weather_df.csv'\n",
    "field_csv_path = 'sampled_field_df.csv'\n",
    "\n",
    "# Delete sampled_weather_df.csv if it exists\n",
    "if os.path.exists(weather_csv_path):\n",
    "    os.remove(weather_csv_path)\n",
    "    print(f\"Deleted {weather_csv_path}\")\n",
    "else:\n",
    "    print(f\"{weather_csv_path} does not exist.\")\n",
    "\n",
    "# Delete sampled_field_df.csv if it exists\n",
    "if os.path.exists(field_csv_path):\n",
    "    os.remove(field_csv_path)\n",
    "    print(f\"Deleted {field_csv_path}\")\n",
    "else:\n",
    "    print(f\"{field_csv_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca09816",
   "metadata": {},
   "source": [
    "# Validating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb7c6cc",
   "metadata": {},
   "source": [
    "Before the  the analysis part, its good to notice how much simpler this data import is now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ed490",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from field_data_processor import FieldDataProcessor\n",
    "from weather_data_processor import WeatherDataProcessor\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params ={\n",
    "    \"sql_query\": \"\"\"\n",
    "            SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "            , # Insert your SQL query\n",
    "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db', # Insert the db_path of the database\n",
    "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'},# Insert the disctionary of columns we want to swop the names of, \n",
    "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}, # Insert the croptype renaming dictionary\n",
    "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the weather data CSV here\n",
    "    \"weather_mapping_csv\":\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\", # Insert the weather data mapping CSV here\n",
    "\n",
    "\n",
    "    # Paste in your previous dictionary data in here\n",
    "\n",
    "    # Add two new keys\n",
    "   \"weather_csv_path\":\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\",\n",
    "# Insert the URL for the weather station data\n",
    "    \"regex_patterns\" : {\n",
    "                        'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "                         'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "                        'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'} ,\n",
    "\n",
    "}   # Paste in your previous dictionary data in here\n",
    "\n",
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "field_df = field_processor.df\n",
    "\n",
    "weather_processor = WeatherDataProcessor(config_params)\n",
    "weather_processor.process()\n",
    "weather_df = weather_processor.weather_df\n",
    "\n",
    "# Rename 'Ave_temps' in field_df to 'Temperature' to match weather_df\n",
    "field_df.rename(columns={'Ave_temps': 'Temperature'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f7e9d8",
   "metadata": {},
   "source": [
    "\n",
    "Back to the initial plan:\n",
    "\n",
    "1. Create a null hypothesis.\n",
    "1. Import the `MD_agric_df` dataset and clean it up.\n",
    "1. Import the weather data.\n",
    "1. Map the weather data to the field data.\n",
    "1. Calculate the means of the weather station dataset and the means of the main dataset.\n",
    "2. Calculate all the parameters we need to do a t-test. \n",
    "3. Interpret our results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbcc03",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "###  this part ...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b650117",
   "metadata": {},
   "source": [
    "Knowing  if our field data is representing the reality in Maji Ndogo by looking at an independent set of data. If field data (means) are the same as the weather data (means), then it indicates no significant difference between the datasets. \n",
    "\n",
    "<br>\n",
    "\n",
    "Given a significance level $\\alpha$ of 0.05 for a two-tailed test, hypothesis test at a 95% confidence interval:\n",
    "\n",
    "- $H_0$: There is no significant difference between the means of the two datasets. This is expressed as $\\mu_{field} = \\mu_{weather}$.\n",
    "\n",
    "- $H_a$: There is a significant difference between the means of the two datasets. This is expressed as $\\mu_{field} \\neq \\mu_{weather}$.\n",
    "\n",
    "<br>\n",
    "\n",
    "If the p-value obtained from the test:\n",
    "- is less than or equal to the significance level, so $p \\leq \\alpha$, we reject the null hypothesis.\n",
    "- is larger than the significance level, so $p > \\alpha$, we cannot reject the null hypothesis, as we cannot find a statistically significant difference between the datasets at the 95% confidence level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1bf54",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First,  to import all of the packages and define a few variables.  importing a new method, `.ttest_ind()`. This method takes in two data columns and calculates means, variance, and returns the the t- and p-statistics. So t-test is reduced to one line. Since the alternative hypothesis does not make a claim of greater or less than, we will use the two-sided t-test, by adding the `alternative = 'two-sided'` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5833b8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "\n",
    "# Now, the measurements_to_compare can directly use 'Temperature', 'Rainfall', and 'Pollution_level'\n",
    "measurements_to_compare = ['Temperature', 'Rainfall', 'Pollution_level']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f085a",
   "metadata": {},
   "source": [
    " compare the means of the temperature, rainfall, and pollution data, for fields assigned to a specific weather station. So for both datasets, one need to isolate the measurement type and weather station for each data, and  comparing the correct means.\n",
    "\n",
    "1. filtering both `field_df` and `weather_df` based on the given station ID and measurement using `filter_field_data(df, station_id, measurement)` and `filter_weather_data(df, station_id, measurement)`.  \n",
    "2. We need to performing  a t-test to conduct the t-test on the filtered data. using `ttest_ind(data_col1, data_col2, equal_var=False)` from `scipy.stats`.\n",
    "3. `print_ttest_results(station_id, measurement, p_val, alpha)` to interpret and print the results from the t-test.\n",
    "\n",
    "defining  these functions, focusing on `Temperature` for `station ID = 0`. Then, integrate these functions into a loop that iterates over each station ID and measurement type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fe83fc",
   "metadata": {},
   "source": [
    "Creating a `filter_field_data` function that takes in the `field_df` DataFrame, the `station_id`, and `measurement` type, and returns a single column (series) of data filtered by the `station_id`, and `measurement`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b52be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_field_data(df, station_id, measurement):\n",
    "    from scipy.stats import ttest_ind\n",
    "    import numpy as np\n",
    "\n",
    "# Now, the measurements_to_compare can directly use 'Temperature', 'Rainfall', and 'Pollution_level'\n",
    "    measurements_to_compare = ['Temperature', 'Rainfall', 'Pollution_level']\n",
    "    \n",
    "    \n",
    "    return df[(df['Weather_station'] == station_id) & (df[measurement].notnull())][measurement] \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ae3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "alpha = 0.05\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "field_values = filter_field_data(field_df, station_id, measurement)\n",
    "field_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99f9178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1375,), First value: 13.35 \n"
     ]
    }
   ],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "alpha = 0.05\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "field_values = filter_field_data(field_df, station_id, measurement)\n",
    "print(f\"Shape: {field_values.shape}, First value: {field_values.iloc[0]} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff140a",
   "metadata": {},
   "source": [
    "`Shape: (1375,), First value: 13.35 `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6ac4c9",
   "metadata": {},
   "source": [
    "a data filter function that takes in the `weather_df` DataFrame, the `station_id`, and `measurement` type, and returns a **single column** (series) of data filtered by the `station_id`, and `measurement`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50110261",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_weather_data(df, station_id, measurement):\n",
    "     \"\"\"\n",
    "    Filter weather data DataFrame to include only rows where 'Weather_station_ID' is equal to station_id,\n",
    "    and 'Measurement' is equal to measurement, then return the 'Value' column.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing weather data.\n",
    "    - station_id (str): The ID of the weather station to filter by.\n",
    "    - measurement (str): The name of the measurement to filter by.\"\"\"\n",
    "\n",
    "     return df[(df['Weather_station_ID'] == station_id) & (df['Measurement'] == measurement)]['Value']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a948c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "alpha = 0.05\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "\n",
    "weather_values = filter_weather_data(weather_df, station_id, measurement)\n",
    "weather_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e8064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "alpha = 0.05\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "\n",
    "weather_values = filter_weather_data(weather_df, station_id, measurement)\n",
    "\n",
    "print(f\"Shape: {weather_values.shape}, First value: {weather_values.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb1692",
   "metadata": {},
   "source": [
    " a function that calculates the t-statistic and p-value. The function should accept two **single columns** of data and return a tuple of the t-statistic and p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_ttest(Column_A, Column_B):\n",
    "    from scipy.stats import ttest_ind\n",
    "    t_statistic, p_value = ttest_ind(Column_A, Column_B, equal_var=False)\n",
    "    return t_statistic, p_value  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b07adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "alpha = 0.05\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "field_values = filter_field_data(field_df, station_id, measurement)\n",
    "weather_values = filter_weather_data(weather_df, station_id, measurement)\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_val = run_ttest(field_values, weather_values)\n",
    "print(f\"T-stat: {t_stat:.5f}, p-value: {p_val:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae8fcf",
   "metadata": {},
   "source": [
    "codeto  print out the t-test result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_ttest_results(station_id, measurement, p_val, alpha):\n",
    "    \"\"\"\n",
    "    Interprets and prints the results of a t-test based on the p-value.\n",
    "    \"\"\"\n",
    "    if p_val < alpha:\n",
    "        print(f\"   Significant difference in {measurement} detected at Station  {station_id}, (P-Value: {p_val:.5f} < {alpha}). Null hypothesis rejected.\")\n",
    "    else:\n",
    "        print(f\"   No significant difference in {measurement} detected at Station  {station_id}, (P-Value: {p_val:.5f} > {alpha}). Null hypothesis not rejected.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2651a66d",
   "metadata": {},
   "source": [
    "**Input:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59929bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "field_values = filter_field_data(field_df, station_id, measurement)\n",
    "weather_values = filter_weather_data(weather_df, station_id, measurement)\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_val = run_ttest(field_values, weather_values)\n",
    "print_ttest_results(station_id, measurement, p_val, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e22bed",
   "metadata": {},
   "source": [
    "a function that loops over `measurements_to_compare` and all `station_id`, performs a t-test and print the results. The function should accept `field_df`, `weather_df`, `list_measurements_to_compare`, `alpha`. the value of `alpha` should default to a value of 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf6f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hypothesis_results(field_df, weather_df, list_measurements_to_compare, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform hypothesis testing for each weather station and measurement.\n",
    "\n",
    "    Args:\n",
    "        field_df (pandas.DataFrame): DataFrame containing field data.\n",
    "        weather_df (pandas.DataFrame): DataFrame containing weather data.\n",
    "        list_measurements_to_compare (list): List of measurements to compare.\n",
    "        alpha (float, optional): The significance level for hypothesis testing. Defaults to 0.05.\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the results of the hypothesis testing.\n",
    "    \"\"\"\n",
    "    station_ids = sorted(field_df['Weather_station'].unique())\n",
    "    for measurement in list_measurements_to_compare:\n",
    "        for station_id in station_ids:\n",
    "            field_values = filter_field_data(field_df, station_id, measurement)\n",
    "            weather_values = filter_weather_data(weather_df, station_id, measurement)\n",
    "            t_stat, p_val = run_ttest(field_values, weather_values)\n",
    "            print_ttest_results(station_id, measurement, p_val, alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fafd76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "hypothesis_results(field_df, weather_df, measurements_to_compare, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1b7b0",
   "metadata": {},
   "source": [
    "**Expected outcome:**\n",
    "```python \n",
    "   No significant difference in Temperature detected at Station 0, (P-Value: 0.90761 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Rainfall detected at Station 0, (P-Value: 0.21621 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Pollution_level detected at Station 0, (P-Value: 0.56418 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Temperature detected at Station 1, (P-Value: 0.47241 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Rainfall detected at Station 1, (P-Value: 0.54499 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Pollution_level detected at Station 1, (P-Value: 0.24410 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Temperature detected at Station 2, (P-Value: 0.88671 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Rainfall detected at Station 2, (P-Value: 0.36466 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Pollution_level detected at Station 2, (P-Value: 0.99388 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Temperature detected at Station 3, (P-Value: 0.66445 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Rainfall detected at Station 3, (P-Value: 0.39847 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Pollution_level detected at Station 3, (P-Value: 0.15466 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Temperature detected at Station 4, (P-Value: 0.88575 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Rainfall detected at Station 4, (P-Value: 0.33237 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Pollution_level detected at Station 4, (P-Value: 0.21508 > 0.05). Null hypothesis not rejected.\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d75778",
   "metadata": {},
   "source": [
    " This means no evidence to suggest that the weather data is different from the field data. This makes it  confident that our field data, at least in terms of temperature, rainfall, and pollution level is reflecting the reality. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355a713-54a2-4b1e-ad75-87d4b9078dab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
